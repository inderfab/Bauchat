import os
from dotenv import load_dotenv
from ai import get_llm, get_embedding_model

# Im .env file local oder cloud einrichten und die ben√∂tigten Keys eintragen
# f√ºr webseite cloud-modus wird open ai verwendetund speicherung unter aws
# Local kann ein eigenes LLM und ein embedder vorgegeben werden. √ºber huggingface laden


# Schritt 1: .env laden
print("üîÑ Lade .env-Datei...")
load_dotenv()

MODEL_NAME = os.getenv("MODEL_NAME")
MODEL_PATH = os.getenv("MODEL_PATH")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL")

print(f"‚úÖ MODEL_NAME: {MODEL_NAME}")
print(f"‚úÖ MODEL_PATH: {MODEL_PATH}")
print(f"‚úÖ EMBEDDING_MODEL: {EMBEDDING_MODEL}")

# Schritt 2: LLM initialisieren und testen
print("\n‚öôÔ∏è Initialisiere LLM √ºber ai.py...")
try:
    llm_fn = get_llm()  # llm_fn ist eine Funktion!
    print("‚úÖ LLM erfolgreich geladen.\n")

    prompt = "Nenne drei Vorteile von Photovoltaik auf D√§chern."
    print(f"üì® Test-Prompt: {prompt}")

    response, usage = llm_fn(prompt)  # <-- so musst du sie aufrufen
    print("‚úÖ Antwort:")
    print(response)
    print("üìä Usage:")
    print(usage)

except Exception as e:
    print(f"‚ùå Fehler beim Laden oder Ausf√ºhren des LLM: {e}")

# Schritt 3: Embedding-Modell initialisieren und testen
print("\n‚öôÔ∏è Initialisiere Embedding-Modell √ºber ai.py...")
try:
    embedder = get_embedding_model()
    print("‚úÖ Embedding-Modell erfolgreich geladen.")

    sample_text = "Photovoltaikanlagen auf D√§chern leisten einen wichtigen Beitrag zur Energiewende."
    embedding = embedder.embed_query(sample_text)
    print(f"üìà Embedding-Vektor f√ºr Beispieltext (L√§nge {len(embedding)}):")
    print(embedding[:10], "...")  # Nur ersten 10 Werte anzeigen
except Exception as e:
    print(f"‚ùå Fehler beim Embedding: {e}")